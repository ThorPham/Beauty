{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense,Flatten,BatchNormalization,Convolution2D,MaxPooling2D,Activation\n",
    "#from keras.applications.\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.layers import Dropout\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import cv2\n",
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy\n",
    "from os import path\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import random\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import scipy.misc\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()\n",
    "def detectFace(detector,image_path, image_name):\n",
    "    imgAbsPath = os.path.join(image_path,image_name)\n",
    "    img = cv2.imread(imgAbsPath)\n",
    "    faces = detector.detect_faces(img)\n",
    "    resized_im = np.ones((3,3))\n",
    "    if len(faces) == 1:\n",
    "        bbox = faces[0][\"box\"]\n",
    "        x,y,w,h = bbox\n",
    "        (xmin,ymin),(xmax,ymax) = (x,y),(x+w,y+h)\n",
    "        croped_im = img[y:y+h,x:x+w,:]\n",
    "        #print(croped_im.shape)\n",
    "        try :\n",
    "            resized_im = cv2.resize(croped_im, (240,180))\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "#         if resized_im.shape[0] != 224 or resized_im.shape[1] != 224:\n",
    "#             print(\"invalid shape\")\n",
    "\n",
    "        # cv2.imwrite(image_name, resized_im)\n",
    "#     else:\n",
    "#         print(image_name+\" error \" + str(len(faces)))\n",
    "    return resized_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "label_file = \"/home/minhhv/dung/face/SCUT-FBP5500_v2/train_test_files/All_labels.txt\"\n",
    "label = pd.read_csv(label_file,sep=\" \",header=None)\n",
    "label.columns = [\"name_image\",\"score\"]\n",
    "name_images = label[\"name_image\"]\n",
    "scores = label[\"score\"]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "target = []\n",
    "for image,score in zip(name_images,scores):\n",
    "    if image.startswith(\"AF\"):\n",
    "        img = detectFace(detector,\"Images\", image)\n",
    "        #print(img.shape)\n",
    "        #if (img.shape[0]==224) and(img.shape[1]==224):\n",
    "        if (img.shape[0] == 180)and((img.shape[1]) == 240) :\n",
    "            img_flip =  cv2.flip(img,1)\n",
    "            all_image = [img,img_flip]\n",
    "            for im in all_image:\n",
    "                data_train.append(im)\n",
    "                target.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def custom_activation(x):\n",
    "    return K.relu(x,max_value =5)\n",
    "with tf.Session() as ss :\n",
    "    print(ss.run(custom_activation(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg = VGG16(include_top=False, input_shape=(180,240,3))\n",
    "# #vgg.summary()\n",
    "# model = Sequential()\n",
    "# model.add(vgg)\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(128, activation=custom_activation))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(32, activation=custom_activation))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation=custom_activation))\n",
    "# model.layers[0].trainable = True\n",
    "# print (model.summary())\n",
    "\n",
    "# #model.load_weights('model-dropout/model-ldl-resnet-base.h5')\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(180, 240,..., padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \"\"\"\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  if __name__ == '__main__':\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\")`\n",
      "  \n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
      "/home/minhhv/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 180, 240, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 178, 238, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 178, 238, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 178, 238, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 89, 119, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 87, 117, 64)       18496     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 85, 115, 64)       36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 85, 115, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 85, 115, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 42, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 42, 57, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 42, 57, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 42, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 42, 57, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 21, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 19, 26, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 17, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 17, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 17, 24, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 12, 256)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 12, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               3145856   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,324,705\n",
      "Trainable params: 4,323,489\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# uses theano ordering. Note that we leave the image size as None to allow multiple image sizes\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=(180,240,3)))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))          \n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(256, 3, 3))\n",
    "model.add(Convolution2D(256, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_train)/255.\n",
    "y  = np.array(target).reshape((-1,1))/5\n",
    "#y = to_categorical(y,num_classes=5)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3836, 180, 240, 3)\n",
      "(3836, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3068 samples, validate on 768 samples\n",
      "Epoch 1/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0768 - mean_squared_error: 0.0768 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05115, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 2/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05115 to 0.02264, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 3/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0233 - val_mean_squared_error: 0.0233\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02264 to 0.01873, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 6/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0191 - val_mean_squared_error: 0.0191\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01873 to 0.01813, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 8/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01813 to 0.01736, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 9/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01736 to 0.01638, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 10/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0154 - val_mean_squared_error: 0.0154\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01638 to 0.01542, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 11/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01542 to 0.01528, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 12/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01528 to 0.01420, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 13/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01420 to 0.01402, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 15/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0151 - val_mean_squared_error: 0.0151\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0146 - val_mean_squared_error: 0.0146\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0135 - val_mean_squared_error: 0.0135\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01402 to 0.01354, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 18/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0130 - val_mean_squared_error: 0.0130\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01354 to 0.01295, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 19/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0133 - val_mean_squared_error: 0.0133\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0135 - val_mean_squared_error: 0.0135\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0133 - val_mean_squared_error: 0.0133\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01295 to 0.01223, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 25/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01223 to 0.01210, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 27/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0120 - val_mean_squared_error: 0.0120\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01210 to 0.01202, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 29/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01202 to 0.01167, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 30/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01167 to 0.01123, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 31/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0120 - val_mean_squared_error: 0.0120\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.01123 to 0.01100, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 37/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01100 to 0.01077, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 41/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01077 to 0.01040, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 42/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01040 to 0.01025, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 44/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01025 to 0.01021, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 47/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.01021 to 0.00987, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 50/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00987 to 0.00979, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 56/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00979 to 0.00964, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 58/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00964 to 0.00956, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 64/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00956 to 0.00953, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 65/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00953 to 0.00938, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 66/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0096 - mean_squared_error: 0.0096 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00938 to 0.00903, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 68/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00903 to 0.00897, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 74/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00897 to 0.00872, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 76/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00872 to 0.00867, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 77/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00867 to 0.00858, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 80/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00858 to 0.00850, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 88/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00850 to 0.00835, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 97/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00835 to 0.00817, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 101/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 103/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/200\n",
      "3068/3068 [==============================] - 10s 3ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00817 to 0.00805, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 119/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 122/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00805 to 0.00797, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 126/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 136/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 137/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 138/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 139/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 140/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00797 to 0.00795, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 141/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00795 to 0.00790, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 142/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 144/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 145/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 147/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 149/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00790 to 0.00774, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 150/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00774 to 0.00772, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 152/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 153/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 154/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00772 to 0.00770, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 155/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 159/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 160/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00770 to 0.00763, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 161/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 163/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 164/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 166/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 167/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 168/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 169/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 170/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 171/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 172/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 173/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00763 to 0.00753, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 174/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 175/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 176/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 177/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 178/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 179/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00753 to 0.00750, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 180/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 181/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 182/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 183/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 184/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 185/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      "Epoch 186/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 187/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00750 to 0.00750, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 188/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00750 to 0.00738, saving model to model/regression_model_sigmoid_1.h5\n",
      "Epoch 189/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 190/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 191/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 192/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 193/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 194/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 195/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 196/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 197/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 198/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 199/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 200/200\n",
      "3068/3068 [==============================] - 9s 3ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "weight_path = 'model/regression_model_sigmoid_1.h5'\n",
    "ckp = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=32, callbacks=[earlyStopping,ckp], epochs=200,\n",
    "                    verbose=1, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f653a3c38d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "# summarize history for acc\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_mean_squared_error', 'loss', 'mean_squared_error'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,  24.,  90., 337., 565., 354., 395., 228.,   7.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(score_female,bins=10,range=(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_female = []\n",
    "for image,score in zip(name_images,scores):\n",
    "    if image.startswith(\"AF\"):\n",
    "        score_female.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.916667,\n",
       " 3.933333,\n",
       " 2.3,\n",
       " 2.95,\n",
       " 2.583333,\n",
       " 3.1333330000000004,\n",
       " 3.166667,\n",
       " 2.45,\n",
       " 2.666667,\n",
       " 3.1333330000000004,\n",
       " 3.2833330000000003,\n",
       " 2.683333,\n",
       " 2.683333,\n",
       " 3.0333330000000003,\n",
       " 3.2166669999999997,\n",
       " 3.1333330000000004,\n",
       " 2.85,\n",
       " 3.833333,\n",
       " 2.3833330000000004,\n",
       " 2.1166669999999996,\n",
       " 2.333333,\n",
       " 2.416667,\n",
       " 2.15,\n",
       " 3.816667,\n",
       " 2.7833330000000003,\n",
       " 2.1333330000000004,\n",
       " 3.166667,\n",
       " 3.05,\n",
       " 3.1333330000000004,\n",
       " 3.016667,\n",
       " 2.566667,\n",
       " 3.8,\n",
       " 3.516667,\n",
       " 3.7166669999999997,\n",
       " 2.8833330000000004,\n",
       " 1.95,\n",
       " 2.583333,\n",
       " 2.433333,\n",
       " 2.816667,\n",
       " 2.75,\n",
       " 2.8666669999999996,\n",
       " 3.6333330000000004,\n",
       " 3.4666669999999997,\n",
       " 4.0,\n",
       " 3.166667,\n",
       " 2.916667,\n",
       " 4.283333,\n",
       " 2.1166669999999996,\n",
       " 2.7833330000000003,\n",
       " 2.766667,\n",
       " 2.766667,\n",
       " 3.516667,\n",
       " 3.8833330000000004,\n",
       " 3.65,\n",
       " 3.6166669999999996,\n",
       " 2.566667,\n",
       " 2.566667,\n",
       " 2.0,\n",
       " 3.85,\n",
       " 4.133333,\n",
       " 2.2166669999999997,\n",
       " 3.5,\n",
       " 2.7,\n",
       " 4.066667,\n",
       " 2.833333,\n",
       " 2.516667,\n",
       " 3.683333,\n",
       " 3.3666669999999996,\n",
       " 2.2833330000000003,\n",
       " 2.3666669999999996,\n",
       " 3.6,\n",
       " 2.95,\n",
       " 2.766667,\n",
       " 2.6166669999999996,\n",
       " 2.8666669999999996,\n",
       " 3.066667,\n",
       " 2.233333,\n",
       " 4.3333330000000005,\n",
       " 3.2,\n",
       " 3.5,\n",
       " 4.316667,\n",
       " 3.066667,\n",
       " 3.666667,\n",
       " 1.033333,\n",
       " 3.316667,\n",
       " 4.3333330000000005,\n",
       " 3.333333,\n",
       " 2.083333,\n",
       " 2.7833330000000003,\n",
       " 3.8833330000000004,\n",
       " 2.933333,\n",
       " 3.95,\n",
       " 2.5333330000000003,\n",
       " 2.233333,\n",
       " 2.2,\n",
       " 4.033333,\n",
       " 2.5333330000000003,\n",
       " 3.25,\n",
       " 2.65,\n",
       " 3.0333330000000003,\n",
       " 3.2,\n",
       " 2.683333,\n",
       " 3.6333330000000004,\n",
       " 2.066667,\n",
       " 4.2,\n",
       " 4.1,\n",
       " 2.55,\n",
       " 1.8833330000000001,\n",
       " 2.316667,\n",
       " 3.75,\n",
       " 3.4,\n",
       " 4.016667,\n",
       " 3.066667,\n",
       " 3.8833330000000004,\n",
       " 3.05,\n",
       " 3.816667,\n",
       " 2.1,\n",
       " 3.833333,\n",
       " 2.75,\n",
       " 2.3666669999999996,\n",
       " 3.55,\n",
       " 2.316667,\n",
       " 4.216667,\n",
       " 2.2833330000000003,\n",
       " 2.516667,\n",
       " 2.2833330000000003,\n",
       " 2.4666669999999997,\n",
       " 2.6166669999999996,\n",
       " 3.983333,\n",
       " 2.083333,\n",
       " 3.3833330000000004,\n",
       " 3.35,\n",
       " 2.833333,\n",
       " 2.516667,\n",
       " 2.4666669999999997,\n",
       " 2.7166669999999997,\n",
       " 2.8,\n",
       " 3.916667,\n",
       " 2.733333,\n",
       " 3.2166669999999997,\n",
       " 3.666667,\n",
       " 2.2166669999999997,\n",
       " 2.816667,\n",
       " 2.3833330000000004,\n",
       " 1.566667,\n",
       " 3.1333330000000004,\n",
       " 3.05,\n",
       " 3.85,\n",
       " 3.016667,\n",
       " 2.35,\n",
       " 2.6,\n",
       " 3.066667,\n",
       " 4.5,\n",
       " 3.05,\n",
       " 2.416667,\n",
       " 3.25,\n",
       " 2.666667,\n",
       " 2.85,\n",
       " 2.316667,\n",
       " 3.916667,\n",
       " 3.9666669999999997,\n",
       " 1.7,\n",
       " 3.1166669999999996,\n",
       " 2.3833330000000004,\n",
       " 4.25,\n",
       " 2.433333,\n",
       " 3.833333,\n",
       " 2.233333,\n",
       " 1.316667,\n",
       " 3.433333,\n",
       " 3.183333,\n",
       " 2.516667,\n",
       " 3.95,\n",
       " 3.0,\n",
       " 3.416667,\n",
       " 2.7833330000000003,\n",
       " 2.8,\n",
       " 2.816667,\n",
       " 4.1,\n",
       " 3.733333,\n",
       " 2.1333330000000004,\n",
       " 2.766667,\n",
       " 2.583333,\n",
       " 3.433333,\n",
       " 3.9666669999999997,\n",
       " 3.1166669999999996,\n",
       " 3.0,\n",
       " 3.766667,\n",
       " 4.033333,\n",
       " 1.9833330000000002,\n",
       " 3.3833330000000004,\n",
       " 4.183333,\n",
       " 1.9833330000000002,\n",
       " 3.65,\n",
       " 3.433333,\n",
       " 2.433333,\n",
       " 4.183333,\n",
       " 3.433333,\n",
       " 3.5,\n",
       " 2.3833330000000004,\n",
       " 4.266667,\n",
       " 3.2,\n",
       " 2.5333330000000003,\n",
       " 3.6166669999999996,\n",
       " 3.066667,\n",
       " 3.6166669999999996,\n",
       " 3.2,\n",
       " 2.85,\n",
       " 2.65,\n",
       " 2.3,\n",
       " 2.483333,\n",
       " 2.5,\n",
       " 3.5,\n",
       " 1.616667,\n",
       " 3.516667,\n",
       " 3.05,\n",
       " 3.1,\n",
       " 2.7,\n",
       " 3.766667,\n",
       " 2.6333330000000004,\n",
       " 2.483333,\n",
       " 3.483333,\n",
       " 2.3,\n",
       " 2.316667,\n",
       " 3.183333,\n",
       " 2.4666669999999997,\n",
       " 3.166667,\n",
       " 3.65,\n",
       " 2.3666669999999996,\n",
       " 2.9,\n",
       " 2.9666669999999997,\n",
       " 2.2,\n",
       " 1.9666669999999997,\n",
       " 4.033333,\n",
       " 2.583333,\n",
       " 2.9666669999999997,\n",
       " 2.316667,\n",
       " 3.9,\n",
       " 2.7166669999999997,\n",
       " 2.333333,\n",
       " 3.1166669999999996,\n",
       " 2.95,\n",
       " 2.7833330000000003,\n",
       " 2.666667,\n",
       " 3.666667,\n",
       " 3.2833330000000003,\n",
       " 3.083333,\n",
       " 3.583333,\n",
       " 2.7833330000000003,\n",
       " 2.6166669999999996,\n",
       " 2.833333,\n",
       " 2.1333330000000004,\n",
       " 2.516667,\n",
       " 2.2,\n",
       " 3.9,\n",
       " 3.4,\n",
       " 2.7166669999999997,\n",
       " 3.5,\n",
       " 3.3833330000000004,\n",
       " 2.183333,\n",
       " 3.566667,\n",
       " 4.45,\n",
       " 2.8666669999999996,\n",
       " 2.833333,\n",
       " 2.85,\n",
       " 2.6166669999999996,\n",
       " 2.816667,\n",
       " 4.566667,\n",
       " 3.083333,\n",
       " 2.2833330000000003,\n",
       " 2.583333,\n",
       " 2.433333,\n",
       " 2.65,\n",
       " 1.666667,\n",
       " 2.766667,\n",
       " 2.5333330000000003,\n",
       " 2.766667,\n",
       " 3.7833330000000003,\n",
       " 2.316667,\n",
       " 4.05,\n",
       " 2.733333,\n",
       " 2.7,\n",
       " 2.066667,\n",
       " 1.833333,\n",
       " 3.516667,\n",
       " 2.1166669999999996,\n",
       " 2.0333330000000003,\n",
       " 3.166667,\n",
       " 2.9,\n",
       " 2.9666669999999997,\n",
       " 3.833333,\n",
       " 2.6,\n",
       " 2.6166669999999996,\n",
       " 2.916667,\n",
       " 2.2166669999999997,\n",
       " 2.316667,\n",
       " 4.0,\n",
       " 1.75,\n",
       " 2.183333,\n",
       " 1.466667,\n",
       " 3.7166669999999997,\n",
       " 2.65,\n",
       " 2.766667,\n",
       " 2.2,\n",
       " 4.366667,\n",
       " 2.666667,\n",
       " 2.25,\n",
       " 2.9666669999999997,\n",
       " 3.9,\n",
       " 3.35,\n",
       " 2.3,\n",
       " 2.2166669999999997,\n",
       " 3.3833330000000004,\n",
       " 4.4166669999999995,\n",
       " 4.0,\n",
       " 4.0833330000000005,\n",
       " 3.9666669999999997,\n",
       " 3.766667,\n",
       " 2.8,\n",
       " 3.433333,\n",
       " 3.6,\n",
       " 1.8833330000000001,\n",
       " 4.1666669999999995,\n",
       " 2.8666669999999996,\n",
       " 4.033333,\n",
       " 2.3666669999999996,\n",
       " 3.7,\n",
       " 2.8833330000000004,\n",
       " 2.233333,\n",
       " 3.566667,\n",
       " 2.55,\n",
       " 3.3666669999999996,\n",
       " 3.3666669999999996,\n",
       " 3.05,\n",
       " 3.7166669999999997,\n",
       " 3.233333,\n",
       " 2.3,\n",
       " 3.833333,\n",
       " 3.833333,\n",
       " 1.316667,\n",
       " 2.2,\n",
       " 4.216667,\n",
       " 2.666667,\n",
       " 2.083333,\n",
       " 2.65,\n",
       " 2.916667,\n",
       " 3.6166669999999996,\n",
       " 4.316667,\n",
       " 2.6,\n",
       " 2.45,\n",
       " 2.95,\n",
       " 2.016667,\n",
       " 2.916667,\n",
       " 2.2166669999999997,\n",
       " 2.6,\n",
       " 2.583333,\n",
       " 3.333333,\n",
       " 4.316667,\n",
       " 3.1,\n",
       " 2.6333330000000004,\n",
       " 3.05,\n",
       " 4.066667,\n",
       " 2.8,\n",
       " 3.916667,\n",
       " 2.566667,\n",
       " 4.316667,\n",
       " 3.316667,\n",
       " 2.7,\n",
       " 3.8,\n",
       " 3.45,\n",
       " 3.733333,\n",
       " 3.516667,\n",
       " 3.6333330000000004,\n",
       " 3.766667,\n",
       " 2.9666669999999997,\n",
       " 3.016667,\n",
       " 2.266667,\n",
       " 2.55,\n",
       " 3.916667,\n",
       " 4.2,\n",
       " 2.183333,\n",
       " 3.35,\n",
       " 2.083333,\n",
       " 3.9,\n",
       " 3.766667,\n",
       " 4.1,\n",
       " 2.7833330000000003,\n",
       " 2.25,\n",
       " 2.6,\n",
       " 2.2,\n",
       " 2.3,\n",
       " 3.7,\n",
       " 2.0333330000000003,\n",
       " 2.766667,\n",
       " 3.583333,\n",
       " 2.016667,\n",
       " 3.8833330000000004,\n",
       " 2.733333,\n",
       " 4.1666669999999995,\n",
       " 3.333333,\n",
       " 3.05,\n",
       " 4.133333,\n",
       " 3.2166669999999997,\n",
       " 1.7833330000000003,\n",
       " 2.1333330000000004,\n",
       " 2.916667,\n",
       " 2.933333,\n",
       " 3.7166669999999997,\n",
       " 4.1,\n",
       " 3.983333,\n",
       " 2.2833330000000003,\n",
       " 4.133333,\n",
       " 3.333333,\n",
       " 3.2833330000000003,\n",
       " 2.166667,\n",
       " 2.8833330000000004,\n",
       " 2.766667,\n",
       " 4.1,\n",
       " 2.4,\n",
       " 2.1333330000000004,\n",
       " 2.9666669999999997,\n",
       " 3.083333,\n",
       " 3.6166669999999996,\n",
       " 2.35,\n",
       " 4.066667,\n",
       " 2.65,\n",
       " 2.65,\n",
       " 3.1333330000000004,\n",
       " 2.316667,\n",
       " 2.416667,\n",
       " 4.1,\n",
       " 2.483333,\n",
       " 2.266667,\n",
       " 3.85,\n",
       " 2.7833330000000003,\n",
       " 3.1333330000000004,\n",
       " 2.75,\n",
       " 2.083333,\n",
       " 2.516667,\n",
       " 3.516667,\n",
       " 2.416667,\n",
       " 3.5333330000000003,\n",
       " 2.583333,\n",
       " 2.6,\n",
       " 4.2,\n",
       " 2.4666669999999997,\n",
       " 1.833333,\n",
       " 2.6333330000000004,\n",
       " 2.7166669999999997,\n",
       " 4.116667,\n",
       " 1.616667,\n",
       " 3.2,\n",
       " 3.8,\n",
       " 1.533333,\n",
       " 3.916667,\n",
       " 4.1,\n",
       " 3.0333330000000003,\n",
       " 3.1,\n",
       " 2.333333,\n",
       " 2.5,\n",
       " 3.2166669999999997,\n",
       " 3.9666669999999997,\n",
       " 4.466667,\n",
       " 3.1,\n",
       " 2.4666669999999997,\n",
       " 2.7833330000000003,\n",
       " 3.8666669999999996,\n",
       " 3.65,\n",
       " 2.816667,\n",
       " 2.35,\n",
       " 2.9666669999999997,\n",
       " 2.483333,\n",
       " 4.183333,\n",
       " 4.266667,\n",
       " 3.266667,\n",
       " 2.7833330000000003,\n",
       " 2.7,\n",
       " 2.25,\n",
       " 2.583333,\n",
       " 3.833333,\n",
       " 3.316667,\n",
       " 3.2166669999999997,\n",
       " 2.05,\n",
       " 3.2833330000000003,\n",
       " 2.0,\n",
       " 3.35,\n",
       " 2.6,\n",
       " 2.55,\n",
       " 2.7,\n",
       " 4.283333,\n",
       " 1.45,\n",
       " 1.85,\n",
       " 2.5,\n",
       " 3.6333330000000004,\n",
       " 2.8666669999999996,\n",
       " 4.116667,\n",
       " 2.95,\n",
       " 3.566667,\n",
       " 3.4,\n",
       " 2.983333,\n",
       " 3.75,\n",
       " 2.7,\n",
       " 3.9666669999999997,\n",
       " 2.6,\n",
       " 3.3,\n",
       " 3.8833330000000004,\n",
       " 2.7833330000000003,\n",
       " 2.016667,\n",
       " 2.516667,\n",
       " 4.016667,\n",
       " 2.1,\n",
       " 3.6333330000000004,\n",
       " 2.666667,\n",
       " 3.483333,\n",
       " 3.6166669999999996,\n",
       " 2.2833330000000003,\n",
       " 4.116667,\n",
       " 3.233333,\n",
       " 2.9666669999999997,\n",
       " 2.816667,\n",
       " 3.2166669999999997,\n",
       " 2.416667,\n",
       " 2.8,\n",
       " 3.7833330000000003,\n",
       " 3.8,\n",
       " 2.6333330000000004,\n",
       " 3.6166669999999996,\n",
       " 2.583333,\n",
       " 4.216667,\n",
       " 1.633333,\n",
       " 3.9666669999999997,\n",
       " 1.15,\n",
       " 3.433333,\n",
       " 2.583333,\n",
       " 2.816667,\n",
       " 3.1,\n",
       " 3.75,\n",
       " 4.133333,\n",
       " 2.9,\n",
       " 2.5333330000000003,\n",
       " 3.983333,\n",
       " 1.7166669999999997,\n",
       " 3.9666669999999997,\n",
       " 4.116667,\n",
       " 4.0,\n",
       " 2.9,\n",
       " 2.05,\n",
       " 2.25,\n",
       " 1.8166669999999998,\n",
       " 1.7,\n",
       " 2.9666669999999997,\n",
       " 2.0333330000000003,\n",
       " 1.15,\n",
       " 3.7833330000000003,\n",
       " 4.116667,\n",
       " 3.083333,\n",
       " 3.3,\n",
       " 2.233333,\n",
       " 3.266667,\n",
       " 3.0333330000000003,\n",
       " 2.916667,\n",
       " 2.7833330000000003,\n",
       " 3.5,\n",
       " 2.1333330000000004,\n",
       " 2.35,\n",
       " 4.05,\n",
       " 3.2166669999999997,\n",
       " 2.666667,\n",
       " 4.0,\n",
       " 2.65,\n",
       " 3.3,\n",
       " 3.8,\n",
       " 2.05,\n",
       " 2.933333,\n",
       " 2.6,\n",
       " 2.8,\n",
       " 2.3833330000000004,\n",
       " 3.95,\n",
       " 4.233333,\n",
       " 2.8,\n",
       " 2.483333,\n",
       " 2.8833330000000004,\n",
       " 4.1666669999999995,\n",
       " 2.8,\n",
       " 2.683333,\n",
       " 2.333333,\n",
       " 3.8833330000000004,\n",
       " 2.2833330000000003,\n",
       " 2.5,\n",
       " 1.4,\n",
       " 2.3,\n",
       " 2.8666669999999996,\n",
       " 1.283333,\n",
       " 3.833333,\n",
       " 3.7,\n",
       " 3.55,\n",
       " 3.2166669999999997,\n",
       " 2.416667,\n",
       " 2.3833330000000004,\n",
       " 3.983333,\n",
       " 2.8,\n",
       " 2.8,\n",
       " 3.316667,\n",
       " 3.916667,\n",
       " 3.7166669999999997,\n",
       " 4.233333,\n",
       " 2.6166669999999996,\n",
       " 3.7166669999999997,\n",
       " 2.3833330000000004,\n",
       " 4.0833330000000005,\n",
       " 2.6166669999999996,\n",
       " 2.2,\n",
       " 3.983333,\n",
       " 3.55,\n",
       " 2.3666669999999996,\n",
       " 2.016667,\n",
       " 2.733333,\n",
       " 3.4666669999999997,\n",
       " 2.3833330000000004,\n",
       " 3.55,\n",
       " 2.9,\n",
       " 3.933333,\n",
       " 4.35,\n",
       " 4.2,\n",
       " 2.566667,\n",
       " 3.7166669999999997,\n",
       " 2.25,\n",
       " 3.1166669999999996,\n",
       " 3.666667,\n",
       " 2.266667,\n",
       " 3.766667,\n",
       " 3.983333,\n",
       " 2.95,\n",
       " 4.016667,\n",
       " 2.55,\n",
       " 2.516667,\n",
       " 3.083333,\n",
       " 3.9,\n",
       " 2.9,\n",
       " 3.9,\n",
       " 4.3,\n",
       " 4.016667,\n",
       " 2.933333,\n",
       " 2.2833330000000003,\n",
       " 3.75,\n",
       " 2.85,\n",
       " 2.666667,\n",
       " 3.7166669999999997,\n",
       " 3.833333,\n",
       " 3.066667,\n",
       " 3.266667,\n",
       " 2.483333,\n",
       " 1.633333,\n",
       " 2.933333,\n",
       " 2.983333,\n",
       " 2.7833330000000003,\n",
       " 4.3333330000000005,\n",
       " 3.516667,\n",
       " 3.25,\n",
       " 3.8,\n",
       " 3.15,\n",
       " 3.066667,\n",
       " 2.683333,\n",
       " 3.016667,\n",
       " 1.833333,\n",
       " 3.666667,\n",
       " 2.766667,\n",
       " 2.733333,\n",
       " 2.2,\n",
       " 2.266667,\n",
       " 2.95,\n",
       " 2.8,\n",
       " 4.233333,\n",
       " 3.666667,\n",
       " 2.266667,\n",
       " 3.8,\n",
       " 2.55,\n",
       " 4.066667,\n",
       " 2.816667,\n",
       " 1.6833330000000002,\n",
       " 4.383333,\n",
       " 4.116667,\n",
       " 2.266667,\n",
       " 2.5333330000000003,\n",
       " 2.816667,\n",
       " 2.3833330000000004,\n",
       " 2.7166669999999997,\n",
       " 4.316667,\n",
       " 3.766667,\n",
       " 3.833333,\n",
       " 2.95,\n",
       " 2.8,\n",
       " 3.3,\n",
       " 2.6,\n",
       " 3.0333330000000003,\n",
       " 3.95,\n",
       " 4.016667,\n",
       " 3.016667,\n",
       " 3.15,\n",
       " 3.3,\n",
       " 3.333333,\n",
       " 3.983333,\n",
       " 2.516667,\n",
       " 2.75,\n",
       " 2.566667,\n",
       " 2.6333330000000004,\n",
       " 2.9,\n",
       " 2.8666669999999996,\n",
       " 4.033333,\n",
       " 3.9,\n",
       " 3.7,\n",
       " 3.233333,\n",
       " 2.6166669999999996,\n",
       " 2.25,\n",
       " 4.066667,\n",
       " 3.45,\n",
       " 4.1666669999999995,\n",
       " 4.1666669999999995,\n",
       " 2.55,\n",
       " 1.166667,\n",
       " 2.8666669999999996,\n",
       " 2.166667,\n",
       " 2.7833330000000003,\n",
       " 1.5,\n",
       " 2.55,\n",
       " 3.683333,\n",
       " 3.5,\n",
       " 2.666667,\n",
       " 2.666667,\n",
       " 2.916667,\n",
       " 4.066667,\n",
       " 2.95,\n",
       " 3.9666669999999997,\n",
       " 3.933333,\n",
       " 3.166667,\n",
       " 3.65,\n",
       " 2.6333330000000004,\n",
       " 3.65,\n",
       " 2.733333,\n",
       " 2.95,\n",
       " 2.4,\n",
       " 2.433333,\n",
       " 2.9666669999999997,\n",
       " 2.7,\n",
       " 2.75,\n",
       " 3.7,\n",
       " 3.6333330000000004,\n",
       " 3.5,\n",
       " 2.433333,\n",
       " 3.183333,\n",
       " 1.8833330000000001,\n",
       " 2.5,\n",
       " 2.016667,\n",
       " 3.266667,\n",
       " 3.016667,\n",
       " 4.15,\n",
       " 4.066667,\n",
       " 3.816667,\n",
       " 1.6,\n",
       " 2.733333,\n",
       " 2.766667,\n",
       " 4.016667,\n",
       " 2.1166669999999996,\n",
       " 2.583333,\n",
       " 2.1166669999999996,\n",
       " 2.8,\n",
       " 3.35,\n",
       " 2.316667,\n",
       " 4.016667,\n",
       " 3.3666669999999996,\n",
       " 1.85,\n",
       " 3.8666669999999996,\n",
       " 2.8,\n",
       " 2.6,\n",
       " 1.9,\n",
       " 3.683333,\n",
       " 3.933333,\n",
       " 4.2,\n",
       " 2.916667,\n",
       " 2.9,\n",
       " 2.0333330000000003,\n",
       " 4.0,\n",
       " 2.583333,\n",
       " 2.933333,\n",
       " 4.266667,\n",
       " 2.2833330000000003,\n",
       " 2.666667,\n",
       " 2.3,\n",
       " 3.566667,\n",
       " 3.916667,\n",
       " 4.2,\n",
       " 2.8,\n",
       " 2.4666669999999997,\n",
       " 2.2833330000000003,\n",
       " 3.1166669999999996,\n",
       " 3.233333,\n",
       " 2.833333,\n",
       " 3.35,\n",
       " 3.266667,\n",
       " 2.25,\n",
       " 2.566667,\n",
       " 1.383333,\n",
       " 2.933333,\n",
       " 1.9333330000000002,\n",
       " 2.666667,\n",
       " 1.6833330000000002,\n",
       " 2.9666669999999997,\n",
       " 3.816667,\n",
       " 3.066667,\n",
       " 2.733333,\n",
       " 2.266667,\n",
       " 3.683333,\n",
       " 3.4666669999999997,\n",
       " 2.233333,\n",
       " 2.4666669999999997,\n",
       " 4.25,\n",
       " 2.8833330000000004,\n",
       " 2.333333,\n",
       " 2.4666669999999997,\n",
       " 2.5333330000000003,\n",
       " 2.516667,\n",
       " 4.233333,\n",
       " 3.6,\n",
       " 1.8666669999999999,\n",
       " 2.9666669999999997,\n",
       " 3.1333330000000004,\n",
       " 2.583333,\n",
       " 3.566667,\n",
       " 2.7166669999999997,\n",
       " 3.183333,\n",
       " 3.4666669999999997,\n",
       " 3.0333330000000003,\n",
       " 3.65,\n",
       " 4.133333,\n",
       " 3.333333,\n",
       " 3.1166669999999996,\n",
       " 3.483333,\n",
       " 4.1,\n",
       " 3.8666669999999996,\n",
       " 3.6166669999999996,\n",
       " 4.033333,\n",
       " 2.3666669999999996,\n",
       " 2.35,\n",
       " 2.3833330000000004,\n",
       " 2.9,\n",
       " 4.0833330000000005,\n",
       " 3.4666669999999997,\n",
       " 3.916667,\n",
       " 3.0,\n",
       " 2.333333,\n",
       " 3.316667,\n",
       " 2.683333,\n",
       " 2.933333,\n",
       " 3.8666669999999996,\n",
       " 2.833333,\n",
       " 3.016667,\n",
       " 3.3666669999999996,\n",
       " 4.1,\n",
       " 1.7833330000000003,\n",
       " 2.683333,\n",
       " 2.9666669999999997,\n",
       " 3.3833330000000004,\n",
       " 2.016667,\n",
       " 3.5333330000000003,\n",
       " 1.833333,\n",
       " 3.0,\n",
       " 2.316667,\n",
       " 1.55,\n",
       " 4.316667,\n",
       " 3.9,\n",
       " 4.216667,\n",
       " 2.8666669999999996,\n",
       " 2.3,\n",
       " 2.9,\n",
       " 2.583333,\n",
       " 2.2166669999999997,\n",
       " 2.8,\n",
       " 3.166667,\n",
       " 2.8,\n",
       " 4.0833330000000005,\n",
       " 3.183333,\n",
       " 3.7166669999999997,\n",
       " 2.766667,\n",
       " 2.3833330000000004,\n",
       " 2.6333330000000004,\n",
       " 2.7,\n",
       " 3.683333,\n",
       " 1.083333,\n",
       " 2.5,\n",
       " 4.3333330000000005,\n",
       " 3.016667,\n",
       " 3.5,\n",
       " 4.4166669999999995,\n",
       " 2.2,\n",
       " 3.35,\n",
       " 2.016667,\n",
       " 3.183333,\n",
       " 1.433333,\n",
       " 2.766667,\n",
       " 2.2833330000000003,\n",
       " 2.4,\n",
       " 2.05,\n",
       " 3.083333,\n",
       " 4.183333,\n",
       " 1.9666669999999997,\n",
       " 2.516667,\n",
       " 3.2,\n",
       " 2.4666669999999997,\n",
       " 1.616667,\n",
       " 2.816667,\n",
       " 3.25,\n",
       " 2.7833330000000003,\n",
       " 3.6166669999999996,\n",
       " 2.833333,\n",
       " 2.95,\n",
       " 1.9666669999999997,\n",
       " 2.7833330000000003,\n",
       " 3.75,\n",
       " 3.833333,\n",
       " 3.5333330000000003,\n",
       " 3.183333,\n",
       " 3.433333,\n",
       " 3.2833330000000003,\n",
       " 3.333333,\n",
       " 3.816667,\n",
       " 2.166667,\n",
       " 4.483333,\n",
       " 4.016667,\n",
       " 2.333333,\n",
       " 2.683333,\n",
       " 3.8833330000000004,\n",
       " 2.316667,\n",
       " 3.916667,\n",
       " 3.683333,\n",
       " 2.083333,\n",
       " 2.8666669999999996,\n",
       " 3.183333,\n",
       " 4.433333,\n",
       " 2.333333,\n",
       " 2.916667,\n",
       " 3.7,\n",
       " 2.3666669999999996,\n",
       " 4.2,\n",
       " 3.6,\n",
       " 4.033333,\n",
       " 2.6166669999999996,\n",
       " 3.7833330000000003,\n",
       " 1.7333330000000002,\n",
       " 3.583333,\n",
       " 3.65,\n",
       " 2.15,\n",
       " 2.1333330000000004,\n",
       " 3.4,\n",
       " 4.05,\n",
       " 2.266667,\n",
       " 4.266667,\n",
       " 2.7833330000000003,\n",
       " 2.5333330000000003,\n",
       " 3.4666669999999997,\n",
       " 3.833333,\n",
       " 2.983333,\n",
       " 3.3833330000000004,\n",
       " 4.2,\n",
       " 3.8,\n",
       " 3.433333,\n",
       " 3.95,\n",
       " 2.3833330000000004,\n",
       " 2.983333,\n",
       " 2.333333,\n",
       " 1.6,\n",
       " 3.9666669999999997,\n",
       " 1.8166669999999998,\n",
       " 4.383333,\n",
       " 2.7,\n",
       " 3.15,\n",
       " 3.0333330000000003,\n",
       " 2.583333,\n",
       " 3.8,\n",
       " 3.183333,\n",
       " 2.6166669999999996,\n",
       " 2.933333,\n",
       " 3.1,\n",
       " 1.7333330000000002,\n",
       " 2.683333,\n",
       " 2.833333,\n",
       " 3.4,\n",
       " 3.683333,\n",
       " 4.0,\n",
       " 3.8666669999999996,\n",
       " 2.766667,\n",
       " 3.016667,\n",
       " 2.35,\n",
       " 4.283333,\n",
       " 2.683333,\n",
       " 2.5333330000000003,\n",
       " 2.55,\n",
       " 2.516667,\n",
       " 2.733333,\n",
       " 3.483333,\n",
       " 3.016667,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
